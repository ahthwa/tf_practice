{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input, answer\n",
    "x = tf.placeholder(dtype = tf.float64, shape=[None, 28 * 28], name = 'input')\n",
    "y_ = tf.placeholder(dtype = tf.float64, shape=[None, 10], name = 'correct')\n",
    "\n",
    "# weight and bias\n",
    "w = tf.Variable(tf.zeros([28 * 28, 10], dtype =tf.float64), name = 'weight')\n",
    "b = tf.Variable(tf.zeros([10], dtype=tf.float64), name = 'bias', dtype=tf.float64)\n",
    "\n",
    "# prediction\n",
    "y = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "\n",
    "# cross entropy\n",
    "# sum of gradient for each training data. reduction_indices = [1]\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), 1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# training loop\n",
    "for i in range(50):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_:batch[1]})\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype = tf.float64, shape = [None, 28*28], name = 'input')\n",
    "y_ = tf.placeholder(dtype = tf.float64, shape = [None, 10], name = 'observation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 1 - 100 neurons\n",
    "w1 = tf.Variable(tf.truncated_normal([28 * 28, 100], stddev=1.0/math.sqrt(float(28*28)), dtype=tf.float64))\n",
    "b1 = tf.Variable(tf.zeros([100], dtype = tf.float64))\n",
    "l1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "\n",
    "# layer 2 - 50 neurones\n",
    "w2 = tf.Variable(tf.truncated_normal([100, 50], stddev=1.0/math.sqrt(float(100)), dtype=tf.float64))\n",
    "b2 = tf.Variable(tf.zeros([50], dtype = tf.float64))\n",
    "l2 = tf.nn.relu(tf.matmul(l1, w2) + b2)\n",
    "\n",
    "# layer 3 - 10 neurons. softmax / output layer\n",
    "w3 = tf.Variable(tf.truncated_normal([50, 10], stddev=1.0/math.sqrt(float(50)), dtype = tf.float64))\n",
    "b3 = tf.Variable(tf.zeros([10], dtype = tf.float64))\n",
    "y = tf.nn.softmax(tf.nn.sigmoid(tf.matmul(l2, w3) + b3))\n",
    "#y = tf.matmul(l2, w3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=1))\n",
    "#cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_)\n",
    "#loss = tf.reduce_mean(cross_entropy, reduction_indices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Setting\n",
    "#train_step = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1]})\n",
    "    #sess.run([train_step], feed_dict={x:batch[0], y_:batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64), reduction_indices=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modeling - define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=[None, 28 * 28], dtype=tf.float32, name='input')\n",
    "y_ = tf.placeholder(shape=[None, 10], dtype=tf.float32, name='label')\n",
    "keep_prb = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('hidden1'):\n",
    "    filter_shape = [5, 5, 1, 32]\n",
    "    w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([32], stddev=0.1)) # why not [14, 14, 32]\n",
    "    x_img = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    l1 = tf.nn.relu(tf.nn.conv2d(x_img, w, [1, 1, 1, 1], 'SAME') + b)\n",
    "    l1 = tf.clip_by_value(l1, 0.0, 10.0)\n",
    "    p1 = tf.nn.max_pool(l1, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "with tf.name_scope('hidden2'):\n",
    "    filter_shape = [5, 5, 32, 64]\n",
    "    w = tf.Variable(tf.truncated_normal(filter_shape, stddev = 0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([64], stddev=0.1))\n",
    "    l2 = tf.nn.relu(tf.nn.conv2d(p1, w, [1, 1, 1, 1], 'SAME') + b)\n",
    "    l2 = tf.clip_by_value(l2, 0.0, 10.0)\n",
    "    p2 = tf.nn.max_pool(l2, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "\n",
    "with tf.name_scope('fully-connected'):\n",
    "    w = tf.Variable(tf.truncated_normal([64 * 7 * 7, 128], stddev=0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([128]))\n",
    "    l2_flat = tf.reshape(p2, [-1, 64 * 7 * 7])\n",
    "    l3 = tf.nn.relu(tf.matmul(l2_flat, w) + b)\n",
    "    l3 = tf.clip_by_value(l3, 0.0, 10.0)\n",
    "\n",
    "with tf.name_scope('softmax'):\n",
    "    w = tf.Variable(tf.truncated_normal([128, 10], stddev=0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([10]))\n",
    "    y = tf.nn.softmax(tf.matmul(tf.nn.dropout(l3, keep_prb), w) + b)\n",
    "    \n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=1), reduction_indices=0)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1)), tf.float32))\n",
    "#training_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "training_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "test_img = mnist.test.images\n",
    "test_lbl = mnist.test.labels\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if (i % 100 == 0):\n",
    "        acc = accuracy.eval(feed_dict={x:batch[0], y_:batch[1], keep_prb:0.5})\n",
    "        cross_ent = cross_entropy.eval(feed_dict={x:batch[0], y_:batch[1]})\n",
    "        print(\"Step: %d, training - cross entropy: %f, accuracy: %f\" %(i, cross_ent, acc))\n",
    "    sess.run([training_step], feed_dict={x:batch[0], y_:batch[1]})\n",
    "\n",
    "acc = accuracy.eval(feed_dict={x:test_img, y_:test_lbl, keep_prb:1.0})\n",
    "cross_ent = cross_entropy.eval(feed_dict={x:test_img, y_:test_lbl, keep_prb:1.0})\n",
    "print(\"Step: %d, test - cross entropy: %f, accuracy: %f\" %(i, cross_ent, acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 결과\n",
    "\n",
    "### Adam Optimizer와 GradientDescentOptimizer\n",
    "\n",
    "이론적인 차이는 찾아봐야겠지만, 실제로 돌렸을 때 gradient descent는 gradient가 nan이 되는 경우가 있는데, weight를 적당히 clipping해 주면 learning을 돌릴 수는 있다.\n",
    "\n",
    "Adam Optimizer의 성능이 훨씬 좋은 것 같다. 단 learning rate parameter가 동일하지는 않다.\n",
    "\n",
    "### 성능 지표\n",
    "\n",
    "```\n",
    "[5x5] convolution, same padding, 32 output, relu, cliping(0,10), [2x2] max pooling\n",
    "[5x5] convolution, same padding, 64 output, relu, cliping(0,10), [2x2] max pooling\n",
    "fully connected, 128 output, relu, cliping(0,10)\n",
    "softmax, drop out, keep prob 0.9\n",
    "Adam(1e-4)\n",
    "\n",
    "Step: 0, training - cross entropy: 3.272329, accuracy: 0.160000\n",
    "Step: 100, training - cross entropy: 0.897650, accuracy: 0.700000\n",
    "Step: 200, training - cross entropy: 0.478571, accuracy: 0.860000\n",
    "Step: 300, training - cross entropy: 0.412178, accuracy: 0.840000\n",
    "Step: 400, training - cross entropy: 0.309230, accuracy: 0.900000\n",
    "Step: 500, training - cross entropy: 0.283316, accuracy: 0.940000\n",
    "Step: 600, training - cross entropy: 0.110163, accuracy: 1.000000\n",
    "Step: 700, training - cross entropy: 0.167554, accuracy: 0.980000\n",
    "Step: 800, training - cross entropy: 0.254424, accuracy: 0.920000\n",
    "Step: 900, training - cross entropy: 0.336519, accuracy: 0.880000\n",
    "Step: 999, test - cross entropy: 0.157881, accuracy: 0.954300\n",
    "```\n",
    "\n",
    "```\n",
    "[5x5] convolution, same padding, 32 output, relu, cliping(0,10), [2x2] max pooling\n",
    "[5x5] convolution, same padding, 64 output, relu, cliping(0,10), [2x2] max pooling\n",
    "fully connected, 128 output, relu, cliping(0,10)\n",
    "softmax, drop out\n",
    "Adam(1e-4)\n",
    "\n",
    "Step: 0, training - cross entropy: 2.676068, accuracy: 0.120000\n",
    "Step: 100, training - cross entropy: 0.716845, accuracy: 0.860000\n",
    "Step: 200, training - cross entropy: 0.443431, accuracy: 0.880000\n",
    "Step: 300, training - cross entropy: 0.226923, accuracy: 0.900000\n",
    "Step: 400, training - cross entropy: 0.233747, accuracy: 0.940000\n",
    "Step: 500, training - cross entropy: 0.267062, accuracy: 0.920000\n",
    "Step: 600, training - cross entropy: 0.097137, accuracy: 0.980000\n",
    "Step: 700, training - cross entropy: 0.181540, accuracy: 0.960000\n",
    "Step: 800, training - cross entropy: 0.130207, accuracy: 0.980000\n",
    "Step: 900, training - cross entropy: 0.105612, accuracy: 0.980000\n",
    "Step: 999, test - cross entropy: 0.132037, accuracy: 0.960200\n",
    "```\n",
    "\n",
    "### 배운 것\n",
    "\n",
    "fully connected 와 달리\n",
    "\n",
    "1. weight matrix가 convolution patch이다. F.C는 [ output neuron x input neuron ] 였다면 CNN은 [patch w, patch h, in-channel, out-channel] 이 weight matrix가 된다.\n",
    "2. input과 weight를 matrix multiplication 하는 대신 fd.nn.conv2d 연산을 수행한다. convolution을 곱하기라고 생각해도 된다\n",
    "3. output이 1차원이 아니므로 bias도 [ output.w, output.h, output channel ] 이렇게 생겼을 것 같지만 그렇지 않다. [ output channel ] 과 같은 1차원 벡터다\n",
    "4. input을 conv layer로 보낼때, conv layer에서 fully connected layer로 넘어갈 때 reshape, flat 해 줄 것\n",
    "5. drop out은 마지막 softmax할 때. evaluate할 때는 keep prob를 1.0으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_x = [[1, 1, 1], [1, 1, 1]]\n",
    "print(tf.reduce_sum(test_x))\n",
    "print(test_x)\n",
    "print(tf.reduce_sum(test_x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensor와 value\n",
    "sess = tf.InteractiveSession()\n",
    "sum_val = sess.run(tf.reduce_sum(test_x, 0))\n",
    "sess.close()\n",
    "print(sum_val)\n",
    "print(tf.reduce_sum(test_x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(w[:,0])\n",
    "print(w[0, :])\n",
    "print(w[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tf.zeros([784, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros([784, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(product)\n",
    "sess = tf.Session()\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as s:\n",
    "    result = s.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "  # Run the 'init' op\n",
    "  sess.run(init_op)\n",
    "  # Print the initial value of 'state'\n",
    "  print(sess.run(state))\n",
    "  # Run the op that updates 'state' and print 'state'.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1 = tf.constant([3.0])\n",
    "input2 = tf.constant([2.0])\n",
    "input3 = tf.constant([5.0])\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = tf.mul(input1, intermed)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)\n",
    "\n",
    "# output:\n",
    "# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
